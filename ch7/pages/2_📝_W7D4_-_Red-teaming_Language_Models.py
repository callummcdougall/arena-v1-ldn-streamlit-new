import os
if not os.path.exists("./images"):
    os.chdir("./ch6")
import re, json
import plotly.io as pio

from st_dependencies import *
styling()

def img_to_html(img_path, width):
    with open("images/" + img_path, "rb") as file:
        img_bytes = file.read()
    encoded = base64.b64encode(img_bytes).decode()
    return f"<img style='width:{width}px;max-width:100%;st-bottom:25px' src='data:image/png;base64,{encoded}' class='img-fluid'>"
def st_image(name, width):
    st.markdown(img_to_html(name, width=width), unsafe_allow_html=True)

def read_from_html(filename):
    filename = f"images/{filename}.html"
    with open(filename) as f:
        html = f.read()
    call_arg_str = re.findall(r'Plotly\.newPlot\((.*)\)', html)[0]
    call_args = json.loads(f'[{call_arg_str}]')
    try:
        plotly_json = {'data': call_args[1], 'layout': call_args[2]}
        fig = pio.from_json(json.dumps(plotly_json))
    except:
        del call_args[2]["template"]["data"]["scatter"][0]["fillpattern"]
        plotly_json = {'data': call_args[1], 'layout': call_args[2]}
        fig = pio.from_json(json.dumps(plotly_json))
    return fig

NAMES = []
def complete_fig_dict(fig_dict):
    for name in NAMES:
        if name not in fig_dict:
            fig_dict[name] = read_from_html(name)
    return fig_dict
if "fig_dict" not in st.session_state:
    st.session_state["fig_dict"] = {}
fig_dict_old = st.session_state["fig_dict"]
fig_dict = complete_fig_dict(fig_dict_old)
if len(fig_dict) > len(fig_dict_old):
    st.session_state["fig_dict"] = fig_dict

def section_home():
    st.markdown(r"""
## 1Ô∏è‚É£ Red-teaming Language Models

Yesterday, we looked at vision models, and how to construct adversarial examples for them. Just like for mechanistic interpretability, even though many of the core ideas and intuitions around adversarial attacks are the same for language models, the technical details differ significantly.

In these exercises, which are more open-ended than yesterday's, we'll look at how to red-team language models. We'll start by looking at the paper [Red Teaming Language Models with Language Models](https://arxiv.org/abs/2202.03286), and then we'll try to reproduce some of the experiments in it.
""")

def section_1():
    st.markdown(r"""
## Red-teaming Language Models

In this exercise, you'll red team a chatbot to find inputs where it generates offensive language, reproducing the experimental setup in the [Red Teaming Language Models with Language Models](https://arxiv.org/abs/2202.03286) paper.""")

    st_image("lm-diagram.png", 400)
    st.markdown(r"")
    st.markdown(r"""

This setup involves a **Red LM**, **Target LM**, and **Red Clf** (classifier). The Red LM is used for automatically generating test cases, the Target LM is used for generating replies, and the classifier is used for detecting failing test cases.

First, you should choose which models you'll use. For the Red and Target LMs, we recommend using GPT-2 (or larger). You can do one of the following:
* Work with the GPT-2 model you trained in the transformers chapter
* Import a model from Neel's `TransformerLens` library (see the interpretability chapter to remind yourself how to do this)

You will also need to choose your classifier, to detect offensive or toxic language. We recommend [Unitary‚Äôs BERT-based model](https://huggingface.co/unitary/toxic-bert) or a similar toxicity classifier available on HuggingFace.

Use the zero-shot approach described in the red teaming paper (section 3.1) to generate inputs that elicit offensive language from the chatbot language model. Look for patterns in the failed test cases, to better understand what kinds of inputs the chatbot fails on.

## Bonus exercises

- Use the few-shot, supervised learning, and reinforcement learning approaches (in that order) to generate even harder test cases for the chatbot. How do the test cases generated by each method differ from each other?
- Reproduce various analyses in the red teaming paper, e.g., clustering the test cases, to help find common patterns in the chatbot failures.
""")

func_list = [section_home, section_1]

page_list = ["üè† Home", "1Ô∏è‚É£ Red-teaming Language Models"]
page_dict = {name: idx for idx, name in enumerate(page_list)}

def page():
    with st.sidebar:
        radio = st.radio("Section", page_list)
        st.markdown("---")
    func_list[page_dict[radio]]()

if is_local or check_password():
    page()
